{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preparation for Error Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define The Target Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target_dir = os.listdir(r'C:\\\\Users\\\\Chris.Cirelli\\\\Desktop\\\\Python Programming Docs\\\\GSU\\\\Sprint Project\\\\Legal Docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Personal Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_4_classification_remove_backslashes(Text_file):\n",
    "    '''The purpose of this function is to clean the text files of numerous instances of backslashes \n",
    "    in order to prepare them for the regex expression search. \n",
    "    \n",
    "    Input  =   Single text file \n",
    "    Output =   Single text file cleaned \n",
    "    '''\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    Text_file_lower = Text_file.lower()\n",
    "    \n",
    "    # Split any values in the text on the backslash.  The Text_split_slash should return a list. \n",
    "    Text_split_slash = Text_file_lower.split('\\\\')\n",
    "        \n",
    "    # Return the list to a text. \n",
    "    Text_rejoined = ' '.join(Text_split_slash)\n",
    "                \n",
    "    # Return a list of the cleaned text docs. \n",
    "    return Text_rejoined\n",
    "\n",
    "\n",
    "def clean_text_4_classification_remove_nABC(Text_file):\n",
    "    '''The purpose of this function is to remove the 'n' that appears before words that begin with an upper case letter.  \n",
    "    Input  =   Single txt file\n",
    "    Output =   Clean list of tokens from original txt file\n",
    "   \n",
    "    '''\n",
    "    # Define the regex expression that you want to search for. \n",
    "    Regex_exp = re.compile('n[A-Z*]')\n",
    "    \n",
    "    # Create a list to capture the tokens once they are cleaned \n",
    "    Text_tokenized_cleaned = []\n",
    "            \n",
    "    # Tokenize the given text\n",
    "    Text_tokenized = nltk.word_tokenize(Text_file)\n",
    "            \n",
    "    # Run for loop over tokens for a given text. \n",
    "    for token in Text_tokenized:\n",
    "\n",
    "        # Search for the regex expression\n",
    "        Regex_search = re.search(Regex_exp, token)\n",
    "                \n",
    "        # Test if there was match (None = no match)\n",
    "        if Regex_search != None:\n",
    "                     \n",
    "            # If there was a match, take all letters after the 'n'.   \n",
    "            token_cleaned = token[1:]\n",
    "                    \n",
    "            Text_tokenized_cleaned.append(token_cleaned)\n",
    "                        \n",
    "        # If the Regex_search returned None, return the token back to the Text_tokenized_cleaned list\n",
    "        else:\n",
    "            Text_tokenized_cleaned.append(token)\n",
    "    \n",
    "    # Return a list of clean tokens\n",
    "    return Text_tokenized_cleaned\n",
    "\n",
    "\n",
    "def create_dict_punct():\n",
    "    '''The purpose of this function is to simply create a dictionary of punctuation symbols to use\n",
    "    in other functions'''\n",
    "    import string\n",
    "    Dict = {}\n",
    "    Punct = string.punctuation\n",
    "    for x in Punct:\n",
    "        Dict[x] = ''\n",
    "    return Dict \n",
    "\n",
    "def strip_punctuation(Token_list):\n",
    "    '''The purpose of this function is to strip the punctuation from a list of tokens. \n",
    "    Input  =  List of tokens\n",
    "    Output =  List of tokens absent punctuation.  \n",
    "    '''\n",
    "    \n",
    "    # Import punctuation dictionary\n",
    "    Dict_punct = create_dict_punct()\n",
    "\n",
    "    # Create a list to capture the cleaned tokens\n",
    "    Clean_token_list = []    \n",
    "        \n",
    "    # Iterate over the tokens in the txt file\n",
    "    for x in Token_list:\n",
    "        if x not in Dict_punct:\n",
    "            # Append tokens to clean token list\n",
    "            Clean_token_list.append(x)\n",
    "    \n",
    "    # Return a list of cleaned text\n",
    "    return Clean_token_list\n",
    "\n",
    "def strip_two_letter_words(Token_list):\n",
    "    '''The purpose of this function is to remove any two letter tokens from a list of tokens.\n",
    "    Input  =   List of tokens\n",
    "    Output =   List of tokens absent two letter words'''\n",
    "    \n",
    "    List = [x for x in Token_list if len(x) > 2]\n",
    "    \n",
    "    return List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Target Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clearning_pipeline_Input_4_Error_Checker_Function(Target_directory):\n",
    "    '''The purpose of this function is to prepare text for use with the Error Checker Program\n",
    "    Input  =  Directory list that contains text files\n",
    "    Outpu  =  A list of texts the tokens for which have been cleaned. \n",
    "    '''\n",
    "    \n",
    "    # Identify Text Files in the Target_directory\n",
    "    List_Text_files = [x for x in Target_directory if '.txt' in x]\n",
    "    \n",
    "    # Create List to Capture Clean Text Files\n",
    "    Clean_txt_file_list = []\n",
    "    \n",
    "    # Iterate over text files:\n",
    "    for File in List_Text_files:\n",
    "        \n",
    "        # Open and read files in as txt\n",
    "        File_open = open(File, 'rb')\n",
    "        Text_byte_obj = File_open.read()\n",
    "        Text_str_obj = str(Text_byte_obj)\n",
    "        # Run Clearning Pipeline\n",
    "        txt_strip_backslashes = clean_text_4_classification_remove_backslashes(Text_str_obj)\n",
    "        txt_strip_nABC = clean_text_4_classification_remove_nABC(txt_strip_backslashes)\n",
    "        txt_strip_punct = strip_punctuation(txt_strip_nABC)\n",
    "        txt_strip_2_letter_words = strip_two_letter_words(txt_strip_punct)\n",
    "    \n",
    "        # Append each list of tokens as a txt to the Clean_txt_file_list\n",
    "        Clean_txt_file_list.append(txt_strip_2_letter_words)\n",
    "        \n",
    "    # Return List of clean tokenized text\n",
    "    return Clean_txt_file_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_Text_files = [x for x in Target_dir if '.txt' in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "for File in List_Text_files:\n",
    "    File_open = open(File, 'rb')\n",
    "    Text = File_open.read()\n",
    "    Text_str = str(Text)\n",
    "    txt_strip_backslashes = clean_text_4_classification_remove_backslashes(Text_str)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = text_clearning_pipeline_Input_4_Error_Checker_Function(Target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
